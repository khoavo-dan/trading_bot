{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "This workbook includes all experiments for trading bot algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, typing, os, cv2\n",
    "import utils\n",
    "from collections import deque, namedtuple\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HYPER PARAMETERS and Common assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define of action space\n",
    "action_space = [0, 1, 2]\n",
    "\n",
    "# Common assumption\n",
    "initial_balance=20\n",
    "min_trading = 10\n",
    "trading_fee_rate = 0.001\n",
    "lr = 0.0001\n",
    "epochs = 1\n",
    "normalize_value = 100000\n",
    "# optimizer = Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crypto price file input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['open', 'high', 'low', 'close', 'volume', 'closeTime',\n",
      "       'baseAssetVolume', 'numberOfTrade', 'takerBuyVolume',\n",
      "       'takerBuyBaseVolume',\n",
      "       ...\n",
      "       'RSI', 'STOCH_0', 'STOCH_1', 'STOCHF_0', 'STOCHF_1', 'STOCHRSI_0',\n",
      "       'STOCHRSI_1', 'TRIX', 'ULTOSC', 'WILLR'],\n",
      "      dtype='object', length=114)\n",
      "{'closeTime': 0, 'open': 1, 'high': 2, 'low': 3, 'close': 4, 'volume': 5}\n"
     ]
    }
   ],
   "source": [
    "input = pd.read_pickle(r'C:\\Users\\Dell\\OneDrive\\Documents\\projects\\trading_bot_v0\\Processed_Dataset_from_1Jan22_17Feb24.pkl') # Importing dataset from pickle\n",
    "print(input.columns)\n",
    "criteria = ['closeTime', 'open', 'high', 'low', 'close','volume']\n",
    "mapping = {name:num for num, name in enumerate(criteria)}\n",
    "actions = {0: 'sell', 1: 'hold', 2: 'buy'}\n",
    "print(mapping)\n",
    "series = np.array(input[criteria][-1200:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1: PPO, Actor-Critic Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Actor:\n",
    "#     def __init__(self, input_shape, action_space, lr=None, optimizer=None) -> None:\n",
    "#         self.action_space = action_space\n",
    "#         self.model = Sequential(\n",
    "#             Input(input_shape=input_shape),\n",
    "#             Dense(64, activation=r'relu'),\n",
    "#             Dense(32, activation=r'relu'),\n",
    "#             Dense(action_shape, activation=r'softmax')\n",
    "#         )\n",
    "\n",
    "#     def loss(self, y_true, y_pred):\n",
    "#         # Defined in https://arxiv.org/abs/1707.06347\n",
    "#         advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "#         LOSS_CLIPPING = 0.2\n",
    "#         ENTROPY_LOSS = 0.001\n",
    "\n",
    "#         prob = actions * y_pred\n",
    "#         old_prob = actions * prediction_picks\n",
    "\n",
    "#         ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "\n",
    "#         p1 = ratio * advantages\n",
    "#         p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "        \n",
    "#         actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "#         entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "#         entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "#         total_loss = actor_loss - entropy\n",
    "\n",
    "#         return total_loss\n",
    "    \n",
    "#     def predict(self, state):\n",
    "#         return self.model.predict(state)\n",
    "\n",
    "\n",
    "\n",
    "# class Critic:\n",
    "#     def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "#         self.model = Sequential(\n",
    "#             Input(input_shape=input_shape),\n",
    "#             Dense(64, activation=r'relu'),\n",
    "#             Dense(32, activation=r'relu'),\n",
    "#             Dense(action_shape, activation=r'softmax')\n",
    "#         )\n",
    "\n",
    "#         self.model.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))\n",
    "\n",
    "#     def loss(self, y_true, y_pred):\n",
    "#         value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "#         return value_loss\n",
    "\n",
    "#     def predict(self, state):\n",
    "#         return self.model.predict([state, np.zeros((state.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State and Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(\n",
    "            self, \n",
    "            # Market state\n",
    "            timestamp=0,\n",
    "            series= np.zeros([1,len(mapping)]),\n",
    "            mapping:dict=mapping\n",
    "        ):\n",
    "        market_state = series[timestamp]\n",
    "        self.values = market_state\n",
    "        self.timestamp=0,\n",
    "        self.open = market_state[mapping['open']]\n",
    "        self.high = market_state[mapping['high']]\n",
    "        self.low = market_state[mapping['low']]\n",
    "        self.close = market_state[mapping['close']]\n",
    "        self.volume = market_state[mapping['volume']]\n",
    "        self.shape = len(mapping)\n",
    "    def __repr__(self):\n",
    "        fixed_width = 15\n",
    "        message = '    |'.join(f'{i}: {j:.2f}'.ljust(fixed_width) for i, j in zip(mapping.keys(), self.values))\n",
    "        return message\n",
    "    \n",
    "    def __call__(self):\n",
    "        # return {i:j for i, j in zip(mapping.keys(), self.market_state)}\n",
    "        return list(self.values)\n",
    "\n",
    "class Account:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 state=None,\n",
    "                 initial_balance=initial_balance,\n",
    "                 min_trading=min_trading,\n",
    "                 trading_fee_rate=trading_fee_rate\n",
    "                 ):\n",
    "\n",
    "        price                = state.close\n",
    "\n",
    "        self.initial_balance = initial_balance\n",
    "        self.min_trading     = min_trading\n",
    "        self.trading_fee_rate= trading_fee_rate\n",
    "        self.coin_qty        = 0\n",
    "        self.coin_cost       = 0\n",
    "        self.cash_balance    = initial_balance \n",
    "        self.net_worth       = self.coin_qty * price + self.cash_balance\n",
    "\n",
    "    def _buy(self, price):\n",
    "        if self.cash_balance < self.min_trading:\n",
    "            print(\"Insufficient cash balance to perform buy action.\")\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            hold_qty     = self.coin_qty\n",
    "            avg_price    = self.coin_cost\n",
    "            buy_price    = price * (1 + self.trading_fee_rate)\n",
    "            buy_limit    = self.cash_balance / buy_price\n",
    "            buy_qty      = buy_limit  # Example: buy half of the limit\n",
    "\n",
    "            self.coin_cost    = (hold_qty * avg_price + buy_price * buy_qty) / (hold_qty + buy_qty)\n",
    "            self.coin_qty    += buy_qty\n",
    "            self.cash_balance-= buy_qty * buy_price\n",
    "            self.net_worth    = self.coin_qty * price + self.cash_balance\n",
    "\n",
    "            print(f'Buy {buy_qty:.5f} at {buy_price}')\n",
    "\n",
    "            return True\n",
    "\n",
    "    def _sell(self, price):\n",
    "        # hold_qty     = self.coin_qty\n",
    "        avg_price    = self.coin_cost \n",
    "\n",
    "        sell_price   = price * (1 - self.trading_fee_rate)\n",
    "        sell_qty     = self.coin_qty  # Example: sell half of the holdings\n",
    "\n",
    "        if sell_qty == 0:\n",
    "            return False\n",
    "        else:\n",
    "            self.cash_balance += sell_qty * sell_price\n",
    "            self.coin_qty     -= sell_qty\n",
    "            self.coin_cost     = 0 \n",
    "            self.net_worth     = self.coin_qty * self.coin_cost + self.cash_balance\n",
    "\n",
    "            print(f'Sell {sell_qty:.5f} at {sell_price}')\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'coin_qty: {self.coin_qty:.5f} |coin_cost: {self.coin_cost:.2f} |cash_balance: {self.cash_balance:.2f} |net_worth: {self.net_worth:.2f}' \n",
    "    \n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        return [i for i in self.__dict__.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames, episode):\n",
    "    import pylab\n",
    "    from matplotlib import animation\n",
    "    try:\n",
    "        pylab.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "        patch = pylab.imshow(frames[0])\n",
    "        pylab.axis('off')\n",
    "        pylab.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "        def animate(i):\n",
    "            patch.set_data(frames[i])\n",
    "        anim = animation.FuncAnimation(pylab.gcf(), animate, frames = len(frames), interval=33)\n",
    "        anim.save(str(episode)+'_gameplay.gif')\n",
    "    except:\n",
    "        pylab.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "        patch = pylab.imshow(frames[0])\n",
    "        pylab.axis('off')\n",
    "        pylab.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "        def animate(i):\n",
    "            patch.set_data(frames[i])\n",
    "        anim = animation.FuncAnimation(pylab.gcf(), animate, frames = len(frames), interval=33)\n",
    "        anim.save(str(episode)+'_gameplay.gif', writer=animation.PillowWriter(fps=10))\n",
    "\n",
    "class TradingGraph:\n",
    "    # A crypto trading visualization using matplotlib made to render custom prices which come in following way:\n",
    "    # Date, Open, High, Low, Close, Volume, net_worth, trades\n",
    "    # call render every step\n",
    "    def __init__(self, render_range):\n",
    "        self.Volume = deque(maxlen=render_range)\n",
    "        self.net_worth = deque(maxlen=render_range)\n",
    "        self.render_data = deque(maxlen=render_range)\n",
    "        self.Render_range = render_range\n",
    "\n",
    "        # We are using the style ‘ggplot’\n",
    "        plt.style.use('ggplot')\n",
    "        # close all plots if there are open\n",
    "        plt.close('all')\n",
    "        # figsize attribute allows us to specify the width and height of a figure in unit inches\n",
    "        self.fig = plt.figure(figsize=(16,8)) \n",
    "\n",
    "        # Create top subplot for price axis\n",
    "        self.ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)\n",
    "        \n",
    "        # Create bottom subplot for volume which shares its x-axis\n",
    "        self.ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=self.ax1)\n",
    "        \n",
    "        # Create a new axis for net worth which shares its x-axis with price\n",
    "        self.ax3 = self.ax1.twinx()\n",
    "\n",
    "        # Formatting Date\n",
    "        self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
    "        #self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
    "        \n",
    "        # Add paddings to make graph easier to view\n",
    "        #plt.subplots_adjust(left=0.07, bottom=-0.1, right=0.93, top=0.97, wspace=0, hspace=0)\n",
    "\n",
    "        # we need to set layers\n",
    "        self.ax2.set_xlabel('Date')\n",
    "        self.ax1.set_ylabel('Price')\n",
    "        self.ax3.set_ylabel('Balance')\n",
    "\n",
    "        # I use tight_layout to replace plt.subplots_adjust\n",
    "        self.fig.tight_layout()\n",
    "\n",
    "        # Show the graph with matplotlib\n",
    "        plt.show()\n",
    "\n",
    "    # Render the environment to the screen\n",
    "    def render(self, \n",
    "               state=np.zeros(len(mapping))\n",
    "               ):\n",
    "        # before appending to deque list, need to convert Date to special format\n",
    "        print(state)\n",
    "        Date, Open, High, Low, Close, Volume = state().values()\n",
    "        Date = mpl_dates.date2num([pd.to_datetime(Date)])[0]\n",
    "        print(Date)\n",
    "        self.render_data.append([Date, Open, High, Low, Close])\n",
    "        \n",
    "        # Clear the frame rendered last step\n",
    "        self.ax1.clear()\n",
    "        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='green', colordown='red', alpha=0.8)\n",
    "\n",
    "        # we need to set layers every step, because we are clearing subplots every step\n",
    "        self.ax2.set_xlabel('Date')\n",
    "        self.ax1.set_ylabel('Price')\n",
    "        self.ax3.set_ylabel('Balance')\n",
    "\n",
    "        \"\"\"Display image with matplotlib - interrupting other tasks\"\"\"\n",
    "        # Show the graph without blocking the rest of the program\n",
    "        plt.show(block=False)\n",
    "        # Necessary to view frames before they are unrendered\n",
    "        plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "    def __init__(self, \n",
    "                 data=None, \n",
    "                 window_size=50,\n",
    "                 mapping = mapping,\n",
    "                 action_space=action_space,\n",
    "                 initial_balance:float=initial_balance,\n",
    "                 trading_fee_rate=trading_fee_rate):\n",
    "\n",
    "        # Environment parameters\n",
    "        self.data               = data\n",
    "        self.series             = data\n",
    "        self.length             = len(data)\n",
    "        self.window_size        = int(window_size)\n",
    "        self.action_space       = np.array(action_space)\n",
    "        self.mapping            = mapping\n",
    "        self.initial_balance    = initial_balance\n",
    "        self.min_trading        = 10\n",
    "        self.trading_fee_rate   = trading_fee_rate\n",
    "        self.timestep = 1\n",
    "\n",
    "\n",
    "        # # Create Actor-Critic network model\n",
    "        # self.Actor = Actor(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
    "        # self.Critic = Critic(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
    "\n",
    "\n",
    "        # Orders history contains the balance, net_worth, crypto_bought, crypto_sold, crypto_held values for the last lookback_window_size steps\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, start_index=0, max_timesteps=10):\n",
    "\n",
    "        # self.visualization = TradingGraph(render_range=self.render_range) # init visualization\n",
    "        self.max_timesteps      = max_timesteps\n",
    "        self.timestamp          = 0\n",
    "        self.series             = self.data[start_index:start_index + max_timesteps]\n",
    "        self.state              = State(timestamp=self.timestamp,\n",
    "                                        series=self.series)\n",
    "        self.prev_state         = State(timestamp=self.timestamp,\n",
    "                                        series=self.series)\n",
    "        self.account            = Account(initial_balance=self.initial_balance,\n",
    "                                          state=self.state)\n",
    "        self.goal               = tuple(self.series[-1])\n",
    "        self.done               = len(self.series) <= 1\n",
    "\n",
    "        self.orders_history     = deque(maxlen=self.window_size)\n",
    "        self.market_history     = deque(maxlen=self.window_size)\n",
    "\n",
    "        reward = 0\n",
    "        return self.state.values, reward, self.done\n",
    "    \n",
    "    # create Tensor board writer\n",
    "    def create_writer(self):\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter(comment=\"Trader\")\n",
    "\n",
    "\n",
    "    def update_state(self, timestep=1):\n",
    "        # ADD 1 TO TIMESTAMP AFTER EACH STEP\n",
    "        self.timestamp +=  timestep\n",
    "\n",
    "        # UPDATE THE STATE ACCORDINGLY\n",
    "        self.state      =  State(\n",
    "            self.timestamp,\n",
    "            series=self.series\n",
    "        )\n",
    "\n",
    "        # HISTORY IS THE n LATEST STEP\n",
    "        self.orders_history.append(self.account())\n",
    "        self.market_history.append(self.state())\n",
    "        \n",
    "    def get_reward(self):\n",
    "        # SIMPLE REWARD CALCULATION \n",
    "        prev_net_worth = self.prev_account.net_worth\n",
    "        net_worth = self.account.net_worth\n",
    "        reward = net_worth - prev_net_worth\n",
    "        return reward\n",
    "\n",
    "    def step(self, action, trading_log=False):\n",
    "        price=self.state.close\n",
    "\n",
    "        # UPDATE ACCOUNT CURRENT STATE\n",
    "        self.prev_account = self.account\n",
    "        if action == 0:\n",
    "            pass\n",
    "        elif action == 1 and self.account.cash_balance > self.initial_balance/10000:\n",
    "            self.account._buy(price=price)\n",
    "        elif action == 2 and self.account.coin_qty > 0.000001:\n",
    "            self.account._sell(price=price)\n",
    "\n",
    "        # UPDATE MARKET STATE\n",
    "        self.update_state()\n",
    "\n",
    "        # CALCULATE REWARD\n",
    "        reward = self.get_reward()\n",
    "\n",
    "        # CHECK IF SCENARIO IS DONE\n",
    "        self.done = len(self.series) <= self.max_timesteps\n",
    "\n",
    "        if trading_log:\n",
    "            print(f'Step {self.timestamp}: {actions[action]}\\n {self.account}\\n reward:{reward:.2f}')\n",
    "\n",
    "        return self.state.values, reward, self.done\n",
    "\n",
    "    def render(self, visualize=False):\n",
    "        if visualize:\n",
    "            img = self.visualization.render(state=self.state)\n",
    "        return img\n",
    "    \n",
    "    def random_game(self, \n",
    "                    num_steps=200, \n",
    "                    ):\n",
    "        # average_net_worth = 0\n",
    "        self.reset()\n",
    "        for i in range(num_steps):\n",
    "            action = np.random.randint(3, size=1)[0]\n",
    "            state, reward, done = self.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    def get_advanatage(self, \n",
    "                       rewards, \n",
    "                       dones, \n",
    "                       values, \n",
    "                       next_values, \n",
    "                       gamma = 0.99, \n",
    "                       lamda = 0.95, \n",
    "                       normalize=True):\n",
    "        \"\"\"calculates the Generalized Advantage Estimation (GAE) for a reinforcement learning agent. \n",
    "        GAE is used to reduce the variance of the advantage estimates, \n",
    "        which helps in stabilizing the training process.\n",
    "\n",
    "        Args:\n",
    "            rewards (_type_): _description_\n",
    "            dones (_type_): _description_\n",
    "            values (_type_): _description_\n",
    "            next_values (_type_): _description_\n",
    "            gamma (float, optional): _description_. Defaults to 0.99.\n",
    "            lamda (float, optional): _description_. Defaults to 0.95.\n",
    "            normalize (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        \n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience/Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "# experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.996             # discount factor\n",
    "ALPHA = 1e-3              # learning rate\n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dense2 = layers.Dense(128, activation='relu')\n",
    "        self.value = layers.Dense(1, activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.value(x)\n",
    "\n",
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dense2 = layers.Dense(128, activation='relu')\n",
    "        self.logits = layers.Dense(action_dim, activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return tf.nn.softmax(self.logits(x))\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.states[:]\n",
    "        del self.actions[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "def compute_gae(rewards, values, done_vals, gamma=0.99, lam=0.95):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * (1 - done_vals[i]) - values[i]\n",
    "        gae = delta + gamma * lam * (1 - done_vals[i]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    return advantages\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, eps_clip=0.2, lam=0.95):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.critic = CriticNetwork(state_dim)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.lam = lam\n",
    "        self.policy_old = PolicyNetwork(state_dim, action_dim)\n",
    "        self.policy_old.set_weights(self.policy.get_weights())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = tf.expand_dims(state, axis=0)\n",
    "        probs = self.policy_old(state)\n",
    "        dist = tf.random.categorical(tf.math.log(probs), 1)\n",
    "        action = tf.squeeze(dist, axis=-1).numpy()[0]\n",
    "\n",
    "        return action, tf.math.log(probs[0, action])\n",
    "    \n",
    "    def update(self, experiences):\n",
    "        states, actions, logprobs, rewards, _, done_vals = experiences\n",
    "\n",
    "        # Calculate discounted rewards\n",
    "        discounted_rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_done in zip(reversed(rewards), reversed(done_vals)):\n",
    "            if is_done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            discounted_rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Convert to tensors\n",
    "        rewards = tf.convert_to_tensor(discounted_rewards, dtype=tf.float32)\n",
    "        old_states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        old_actions = actions\n",
    "        old_actions = tf.cast(old_actions, tf.int32)\n",
    "        old_logprobs = tf.convert_to_tensor(logprobs, dtype=tf.float32)\n",
    "\n",
    "        # Get values from the critic\n",
    "        values = self.critic(old_states).numpy()\n",
    "        values = np.append(values, 0)  # Append 0 for the last value\n",
    "\n",
    "        # Compute advantages\n",
    "        advantages = compute_gae(rewards, values, done_vals, self.gamma, self.lam)\n",
    "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "\n",
    "        # Ensure the policy model is built\n",
    "        dummy_input = tf.convert_to_tensor(np.zeros((1, old_states.shape[1])), dtype=tf.float32)\n",
    "        _ = self.policy(dummy_input)\n",
    "\n",
    "        # Training step\n",
    "        with tf.GradientTape() as tape:\n",
    "            probs = self.policy(old_states)\n",
    "            probs = tf.clip_by_value(probs, 1e-10, 1.0)\n",
    "            logprobs = tf.math.log(tf.gather(probs, old_actions, axis=1, batch_dims=1))\n",
    "            dist_entropy = -tf.reduce_sum(probs * tf.math.log(probs), axis=1)\n",
    "            ratios = tf.exp(logprobs - old_logprobs)\n",
    "            surr1 = tf.expand_dims(ratios, axis=1) * advantages\n",
    "            surr2 = tf.expand_dims(tf.clip_by_value(ratios, 1 - self.eps_clip, 1 + self.eps_clip), axis=1) * advantages\n",
    "            loss = -tf.reduce_mean(tf.minimum(surr1, surr2)) + 0.01 * tf.reduce_mean(dist_entropy)\n",
    "            value_loss = tf.reduce_mean(tf.square(rewards - self.critic(old_states)))\n",
    "            total_loss = loss + value_loss\n",
    "\n",
    "        # Apply gradients\n",
    "        grads = tape.gradient(total_loss, self.policy.trainable_variables + self.critic.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy.trainable_variables + self.critic.trainable_variables))\n",
    "\n",
    "        # Copy weights to policy_old\n",
    "        self.policy_old.set_weights(self.policy.get_weights())\n",
    "\n",
    "        # states, actions, logprobs, rewards, _, done_vals = experiences\n",
    "        # rewards = []\n",
    "        # discounted_reward = 0\n",
    "        # for reward, is_terminal in zip(reversed(rewards), reversed(done_vals)):\n",
    "        #     if is_terminal:\n",
    "        #         discounted_reward = 0\n",
    "        #     discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "        #     rewards.insert(0, discounted_reward)\n",
    "        \n",
    "\n",
    "        # # rewards, old_states, old_actions, old_logprobs, *_ = experiences\n",
    "        # rewards         = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        # old_states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        # old_actions     = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        # old_logprobs    = tf.convert_to_tensor(logprobs, dtype=tf.float32)\n",
    "        # values          = self.critic(old_states).numpy()\n",
    "        # values          = np.append(values, 0)  # Append 0 for the last value\n",
    "        \n",
    "        # advantages = compute_gae(rewards, values, self.gamma, self.lam)\n",
    "        # advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "        \n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     logprobs = tf.math.log(tf.gather(self.policy(old_states), old_actions, axis=1, batch_dims=1))\n",
    "        #     dist_entropy = -tf.reduce_sum(self.policy(old_states) * tf.math.log(self.policy(old_states)), axis=1)\n",
    "        #     ratios = tf.exp(logprobs - old_logprobs)\n",
    "        #     print(ratios)\n",
    "        #     print(advantages)\n",
    "        #     surr1 = tf.expand_dims(ratios, axis=1) * advantages\n",
    "        #     surr2 = tf.expand_dims(tf.clip_by_value(ratios, 1-self.eps_clip, 1+self.eps_clip), axis=1) * advantages\n",
    "        #     loss = -tf.reduce_mean(tf.minimum(surr1, surr2)) + 0.01 * tf.reduce_mean(dist_entropy)\n",
    "        #     value_loss = tf.reduce_mean(tf.square(rewards - self.critic(old_states)))\n",
    "        #     total_loss = loss + value_loss\n",
    "        \n",
    "        # grads = tape.gradient(total_loss, self.policy.trainable_variables + self.critic.trainable_variables)\n",
    "        # self.optimizer.apply_gradients(zip(grads, self.policy.trainable_variables + self.critic.trainable_variables))\n",
    "        # self.policy_old.set_weights(self.policy.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_agent.select_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000\n",
      "Episode 717, Reward: 0,\n",
      "Episode 2/1000\n",
      "Episode 780, Reward: 0,\n",
      "Episode 3/1000\n",
      "Episode 962, Reward: 0,\n",
      "Episode 4/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11184\\1037662104.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# print(timestep)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# timestep += 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Select action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[1;31m# print(f'action {action} - logprob {logprob}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# next_state, reward, done = env.step(action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11184\\1803879991.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    586\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 ):\n\u001b[1;32m-> 1149\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11184\\1803879991.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1147\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 ):\n\u001b[1;32m-> 1149\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    237\u001b[0m                 outputs = tf.nn.embedding_lookup_sparse(\n\u001b[0;32m    238\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombiner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                 )\n\u001b[0;32m    240\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[1;31m# Broadcast kernel to inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[0;32m   3838\u001b[0m         \u001b[0madjoint_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madjoint_b\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3839\u001b[0m         return gen_math_ops.batch_mat_mul_v3(\n\u001b[0;32m   3840\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[0;32m   3841\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3842\u001b[1;33m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3843\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n",
      "\u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\findesk_1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6173\u001b[0m         transpose_b)\n\u001b[0;32m   6174\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6175\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6176\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6177\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6178\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6180\u001b[0m       return mat_mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "max_timesteps = 50\n",
    "update_timestep = 100\n",
    "timestep = 0\n",
    "\n",
    "env = TradingEnv(data=series)\n",
    "\n",
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"logprob\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "# TRAINING LOG SETUP\n",
    "# env.create_writer()                # Create TensorBoard writer\n",
    "state_dim = len(mapping)\n",
    "action_dim = len(action_space)\n",
    "\n",
    "# PPO agent and memory buffer\n",
    "ppo_agent = PPO(state_dim, action_dim)\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "episodes = np.random.choice(range(0, env.length-max_timesteps), size=num_episodes, replace=False)\n",
    "\n",
    "for i, episode in enumerate( episodes):\n",
    "    print(f'Episode {i+1}/{num_episodes}')\n",
    "    state, reward, done = env.reset(start_index=episode, max_timesteps=max_timesteps)\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_timesteps-1):\n",
    "        for i in range(50):\n",
    "        # print(timestep)\n",
    "        # timestep += 1\n",
    "\n",
    "        # Select action\n",
    "            action, logprob = ppo_agent.select_action(state)\n",
    "            # print(f'action {action} - logprob {logprob}')\n",
    "            # next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Store experience in memory\n",
    "            memory_buffer.append(experience(state, action, logprob, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update PPO agent\n",
    "            if t+1 % update_timestep == 0:\n",
    "                # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "                experiences = utils.get_experiences(memory_buffer)\n",
    "                ppo_agent.update(experiences)\n",
    "                # timestep = 0\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {i+1}/{num_episodes} - Timestep {t+1}/{max_timesteps} - Net worth {env.net_worth[-1]}\")\n",
    "                break\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Reward: {episode_reward},\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ppo_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mppo_agent\u001b[49m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mget_weights()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ppo_agent' is not defined"
     ]
    }
   ],
   "source": [
    "ppo_agent.policy.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = PPO(state_dim, action_dim)\n",
    "max_timesteps = 26\n",
    "state, reward, done = env.reset(start_index=119, max_timesteps=max_timesteps)\n",
    "for t in range(max_timesteps-1):\n",
    "    # Select action\n",
    "    action, logprob = ppo_agent.select_action(state)\n",
    "    # print(f'action {action} - logprob {logprob}')\n",
    "    next_state, reward, done = env.step(action)\n",
    "\n",
    "    # Store experience in memory\n",
    "    memory_buffer.append(experience(state, action, logprob, reward, next_state, done))\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "states = tf.convert_to_tensor(\n",
    "    [memory_buffer[e].state for e in range(len(memory_buffer)-1)],\n",
    "    dtype='float32'\n",
    "    )\n",
    "\n",
    "actions = tf.convert_to_tensor(\n",
    "    [memory_buffer[e].action for e in range(len(memory_buffer)-1)],\n",
    "    dtype=tf.int32\n",
    "    )\n",
    "\n",
    "logprobs = tf.convert_to_tensor(\n",
    "    [memory_buffer[e].logprob for e in range(len(memory_buffer)-1) ],\n",
    "     dtype=tf.float32)\n",
    "\n",
    "rewards = [memory_buffer[e].reward for e in range(len(memory_buffer)-1)]\n",
    "\n",
    "\n",
    "next_states = tf.convert_to_tensor(\n",
    "    [memory_buffer[e].next_state for e in range(len(memory_buffer)-1)],\n",
    "    dtype=tf.float32\n",
    "    )\n",
    "\n",
    "done_vals = tf.convert_to_tensor(\n",
    "    [np.float32(memory_buffer[e].done) for e in range(len(memory_buffer)-1)],\n",
    "    dtype=tf.float32\n",
    "    )\n",
    "\n",
    "experiences = (states, actions, logprobs, rewards, next_states, done_vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "states, actions, logprobs, rewards, _, done_vals = experiences\n",
    "\n",
    "# Calculate discounted rewards\n",
    "discounted_rewards = []\n",
    "discounted_reward = 0\n",
    "for reward, is_done in zip(reversed(rewards), reversed(done_vals)):\n",
    "    if is_done:\n",
    "        discounted_reward = 0\n",
    "    discounted_reward = reward + (gamma * discounted_reward)\n",
    "    discounted_rewards.insert(0, discounted_reward)\n",
    "\n",
    "# Convert to tensors\n",
    "rewards = tf.convert_to_tensor(discounted_rewards, dtype=tf.float32)\n",
    "old_states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "old_actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "old_logprobs = tf.convert_to_tensor(logprobs, dtype=tf.float32)\n",
    "\n",
    "# Get values from the critic\n",
    "values = ppo_agent.critic(old_states).numpy()\n",
    "values = np.append(values, 0)  # Append 0 for the last value\n",
    "\n",
    "# Compute advantages\n",
    "advantages = compute_gae(rewards, values, done_vals, ppo_agent.gamma, ppo_agent.lam)\n",
    "advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "\n",
    "# Ensure the policy model is built\n",
    "dummy_input = tf.convert_to_tensor(np.zeros((1, old_states.shape[1])), dtype=tf.float32)\n",
    "_ = ppo_agent.policy(dummy_input)\n",
    "\n",
    "# Training step\n",
    "with tf.GradientTape() as tape:\n",
    "    probs = ppo_agent.policy(old_states)\n",
    "    probs = tf.clip_by_value(probs, 1e-10, 1.0)\n",
    "    logprobs = tf.math.log(tf.gather(probs, old_actions, axis=1, batch_dims=1))\n",
    "    dist_entropy = -tf.reduce_sum(probs * tf.math.log(probs), axis=1)\n",
    "    ratios = tf.exp(logprobs - old_logprobs)\n",
    "    surr1 = tf.expand_dims(ratios, axis=1) * advantages\n",
    "    surr2 = tf.expand_dims(tf.clip_by_value(ratios, 1 - ppo_agent.eps_clip, 1 + ppo_agent.eps_clip), axis=1) * advantages\n",
    "    loss = -tf.reduce_mean(tf.minimum(surr1, surr2)) + 0.01 * tf.reduce_mean(dist_entropy)\n",
    "    value_loss = tf.reduce_mean(tf.square(rewards - ppo_agent.critic(old_states)))\n",
    "    total_loss = loss + value_loss\n",
    "\n",
    "# Apply gradients\n",
    "grads = tape.gradient(total_loss, ppo_agent.policy.trainable_variables + ppo_agent.critic.trainable_variables)\n",
    "ppo_agent.optimizer.apply_gradients(zip(grads, ppo_agent.policy.trainable_variables + ppo_agent.critic.trainable_variables))\n",
    "\n",
    "# Copy weights to policy_old\n",
    "ppo_agent.policy_old.set_weights(ppo_agent.policy.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.15601808e-01,  1.84389815e-01,  3.01752985e-03,\n",
       "          5.75450063e-03,  7.34859258e-02, -1.42361224e-01,\n",
       "          2.78608501e-02, -1.42272025e-01,  4.14957255e-02,\n",
       "          1.23044103e-02, -2.18932331e-02, -1.55890614e-01,\n",
       "          3.22590023e-02,  1.67304471e-01, -1.21764243e-02,\n",
       "          1.22337043e-03, -2.05073550e-01, -6.51282072e-03,\n",
       "          2.07127944e-01,  2.86708772e-02,  1.29683778e-01,\n",
       "          9.95540470e-02, -1.48160726e-01, -3.95816565e-02,\n",
       "          1.62010744e-01, -1.87491447e-01,  1.36520788e-01,\n",
       "          4.71553206e-03,  1.79946467e-01,  2.73328424e-02,\n",
       "         -1.76280290e-01,  5.68728596e-02,  5.85278869e-03,\n",
       "          1.38843015e-01,  5.68162054e-02,  1.59480974e-01,\n",
       "          2.00298592e-01, -5.38851023e-02,  7.21125752e-02,\n",
       "         -1.23790801e-02, -1.93298817e-01, -1.49940565e-01,\n",
       "         -1.16506577e-01, -7.58408010e-02, -1.41963169e-01,\n",
       "         -1.71932995e-01, -4.25611436e-02, -1.99298114e-02,\n",
       "         -1.25296682e-01,  2.10477635e-01, -2.08076760e-01,\n",
       "          8.96842927e-02,  2.05669835e-01,  1.86296627e-01,\n",
       "         -1.21593781e-01,  2.41700560e-02, -1.78538844e-01,\n",
       "          7.35378265e-03,  1.81756929e-01, -2.09960565e-01,\n",
       "          1.70428455e-02,  7.83274323e-02, -2.99075693e-02,\n",
       "         -5.19143194e-02, -1.36351496e-01,  2.52975225e-02,\n",
       "          1.69180334e-03, -1.67650372e-01,  1.39134035e-01,\n",
       "          1.03586659e-01, -1.92507565e-01, -1.99613377e-01,\n",
       "          2.51613110e-02,  1.77861348e-01, -1.37084872e-02,\n",
       "          1.23096094e-01, -6.98288530e-02,  3.54300439e-02,\n",
       "         -1.21021070e-01, -2.78823972e-02, -1.04444429e-01,\n",
       "         -6.39815629e-02,  1.63918599e-01,  1.46148816e-01,\n",
       "          2.83610076e-02,  1.35272697e-01, -1.56873018e-02,\n",
       "          1.01576582e-01, -1.57637298e-02, -1.86406165e-01,\n",
       "          2.04138145e-01,  1.40873030e-01, -3.35497707e-02,\n",
       "          1.50443614e-02,  1.96402356e-01, -1.87282830e-01,\n",
       "         -5.85487783e-02,  2.06702307e-01, -9.90720838e-02,\n",
       "         -2.89565325e-02,  1.93742380e-01, -1.02011263e-01,\n",
       "         -1.84806734e-01, -1.79645717e-01,  6.18952960e-02,\n",
       "         -1.23846889e-01,  8.55676979e-02,  1.84705570e-01,\n",
       "         -1.66245580e-01, -2.31146365e-02,  1.06480137e-01,\n",
       "         -1.32062823e-01,  1.61479995e-01, -2.98994482e-02,\n",
       "          8.56199712e-02, -1.22881681e-02, -1.71280310e-01,\n",
       "         -3.90935391e-02, -7.38329887e-02, -7.12613314e-02,\n",
       "         -1.25384957e-01,  1.77163228e-01,  1.35848030e-01,\n",
       "          7.72794932e-02,  1.61732450e-01, -1.51878104e-01,\n",
       "          3.26226503e-02, -3.26256752e-02],\n",
       "        [ 1.79743156e-01,  1.12927124e-01,  1.00168303e-01,\n",
       "         -8.46876055e-02,  4.52598929e-03, -1.68954879e-02,\n",
       "         -5.08522987e-03,  1.20382980e-01, -1.41327396e-01,\n",
       "          2.13782489e-03, -1.66948855e-01, -1.95918143e-01,\n",
       "          7.38560408e-02, -1.06600069e-01,  2.74369121e-02,\n",
       "          1.30643710e-01,  1.79714844e-01,  2.02420071e-01,\n",
       "          3.16943675e-02, -1.69287771e-02,  5.44960946e-02,\n",
       "         -1.29222006e-01, -8.05906355e-02, -1.40885293e-01,\n",
       "          1.40820935e-01,  5.85674495e-02,  1.62509382e-02,\n",
       "         -6.32998347e-02, -1.85247824e-01,  1.11748710e-01,\n",
       "          1.08878165e-02,  2.81374156e-02, -8.52874666e-02,\n",
       "         -2.55937576e-02,  6.17866665e-02,  1.85380057e-01,\n",
       "         -1.72478661e-01, -1.39127523e-01, -1.28227174e-01,\n",
       "          1.43008694e-01, -2.11219564e-01,  1.12232074e-01,\n",
       "          2.09203258e-01, -1.11242346e-01, -5.12604266e-02,\n",
       "         -3.39852571e-02, -1.45227760e-01,  1.66891202e-01,\n",
       "          1.00176200e-01, -5.59362173e-02, -1.22366324e-01,\n",
       "          1.55597046e-01,  1.22912511e-01,  9.29333419e-02,\n",
       "         -1.89162046e-01,  1.58023492e-01,  3.08133513e-02,\n",
       "         -4.96122241e-03,  3.58678550e-02, -8.00257474e-02,\n",
       "          1.30645826e-01,  1.68900564e-01,  6.31140620e-02,\n",
       "          1.21965751e-01,  9.48272645e-03,  1.49871841e-01,\n",
       "          2.03609169e-02,  5.33723682e-02,  1.35670409e-01,\n",
       "          2.08200663e-02,  1.49157271e-01, -1.96759716e-01,\n",
       "         -6.36354089e-03,  4.26548272e-02, -1.71572223e-01,\n",
       "         -5.33607751e-02,  1.90842137e-01, -3.53745520e-02,\n",
       "          7.42708296e-02, -1.82806790e-01, -8.85236412e-02,\n",
       "         -6.90747201e-02,  1.43921003e-01,  1.85043857e-01,\n",
       "          5.81374615e-02, -2.07146451e-01, -1.15457922e-01,\n",
       "         -8.76373351e-02, -2.13928670e-02,  6.45987242e-02,\n",
       "          4.27990109e-02, -9.85464379e-02,  6.96807653e-02,\n",
       "          1.84029087e-01,  9.61200446e-02, -1.15139835e-01,\n",
       "         -1.39000475e-01, -6.92196637e-02,  5.24982661e-02,\n",
       "          7.70614892e-02,  1.48803756e-01,  4.65927869e-02,\n",
       "         -1.09333694e-02,  1.58471897e-01,  1.58656701e-01,\n",
       "         -8.15304369e-02,  1.14810035e-01, -1.75851971e-01,\n",
       "         -5.39468527e-02,  2.09973142e-01,  2.05123916e-01,\n",
       "         -1.70850337e-01,  1.27648264e-02, -6.18770272e-02,\n",
       "         -1.90985426e-01, -1.50265515e-01, -6.99971020e-02,\n",
       "         -1.18532866e-01,  1.82998493e-01, -6.44955039e-02,\n",
       "         -1.08979903e-01,  2.06559107e-01, -6.56370372e-02,\n",
       "         -1.44321516e-01,  2.08053604e-01,  7.35410899e-02,\n",
       "          7.80598968e-02, -1.34514332e-01],\n",
       "        [ 5.09314984e-02, -1.43931687e-01,  2.33220458e-02,\n",
       "         -1.86554477e-01, -3.65842581e-02,  3.63331586e-02,\n",
       "          5.32637686e-02,  1.67046979e-01, -1.49357557e-01,\n",
       "         -1.36098564e-02, -2.00627327e-01, -6.30570054e-02,\n",
       "          8.86544138e-02, -5.55460751e-02,  1.84958741e-01,\n",
       "         -1.76643729e-01, -8.53707492e-02,  2.08435819e-01,\n",
       "         -6.43826872e-02,  1.59095377e-02,  1.73673376e-01,\n",
       "         -5.94137460e-02, -1.51561275e-01,  1.68935850e-01,\n",
       "         -1.75229713e-01, -1.10216893e-01,  9.90456492e-02,\n",
       "         -2.86452472e-02, -1.24564141e-01, -1.50953144e-01,\n",
       "         -4.12499458e-02, -2.62623280e-02,  9.66572315e-02,\n",
       "         -2.02739686e-01, -1.85717508e-01, -1.78984925e-01,\n",
       "         -1.76988810e-01, -5.42852283e-02,  1.64763257e-01,\n",
       "         -1.92338303e-01,  1.34808227e-01,  9.86229628e-02,\n",
       "          2.62587965e-02,  1.64519832e-01,  1.33504018e-01,\n",
       "          1.62328020e-01,  9.18434709e-02, -5.84297180e-02,\n",
       "          1.83796659e-01,  1.80787101e-01, -1.65783614e-02,\n",
       "          1.53279901e-02,  9.99606103e-02,  4.72566038e-02,\n",
       "         -1.90280482e-01, -8.24829787e-02, -1.54443711e-01,\n",
       "         -1.70038179e-01,  1.75773159e-01, -5.37270308e-03,\n",
       "         -1.46754682e-02, -7.84861147e-02, -4.14893776e-02,\n",
       "          1.92904994e-01,  1.54253647e-01, -1.26628578e-02,\n",
       "         -9.78034586e-02,  1.84193805e-01,  1.70418516e-01,\n",
       "          2.04213426e-01,  1.17750421e-01, -1.64594501e-02,\n",
       "          3.24093401e-02,  1.34672895e-01, -1.68462217e-01,\n",
       "         -1.29223123e-01,  7.70100802e-02, -1.23368822e-01,\n",
       "         -1.89950541e-01,  1.55602291e-01,  1.14314511e-01,\n",
       "          7.44973570e-02,  2.05271766e-01, -2.18083858e-02,\n",
       "         -1.49916947e-01,  1.96030661e-01,  9.21904594e-02,\n",
       "         -1.51218325e-01,  1.28072053e-02, -5.10027856e-02,\n",
       "          4.42331582e-02,  2.00100437e-01, -2.24899203e-02,\n",
       "         -3.69500220e-02,  1.58499494e-01, -3.42751890e-02,\n",
       "          9.77687091e-02,  2.09319189e-01,  1.44503117e-02,\n",
       "          2.03487113e-01,  1.66498795e-01, -9.61872861e-02,\n",
       "          4.12934273e-02,  1.31730989e-01, -1.93583712e-01,\n",
       "         -1.77544117e-01,  5.28822690e-02,  1.86767027e-01,\n",
       "          1.32826284e-01,  2.06959590e-01,  8.99755508e-02,\n",
       "         -9.38640535e-02,  5.35711497e-02, -1.77045062e-01,\n",
       "          4.32474762e-02, -2.07642034e-01, -9.78978053e-02,\n",
       "          6.71906918e-02,  1.70818493e-01, -1.02457747e-01,\n",
       "          1.09214857e-01,  7.15780109e-02,  1.28277078e-01,\n",
       "          8.50756615e-02, -1.01080857e-01,  1.42858222e-01,\n",
       "         -9.88671035e-02,  3.02400887e-02],\n",
       "        [-2.49961317e-02, -3.82679254e-02, -1.15989417e-01,\n",
       "          5.87316006e-02,  5.32402545e-02,  1.40306190e-01,\n",
       "          1.22582749e-01,  1.90451726e-01,  1.11155257e-01,\n",
       "          1.83945253e-01,  6.21737391e-02, -1.98323518e-01,\n",
       "          9.97965485e-02, -1.30619481e-01, -1.88099146e-02,\n",
       "         -4.05715853e-02,  1.19491115e-01,  7.42021650e-02,\n",
       "          2.07183436e-01, -9.41813365e-02,  1.06069371e-01,\n",
       "          8.64621997e-03, -1.12585485e-01, -9.72442180e-02,\n",
       "          1.17984667e-01,  4.57158834e-02, -1.58530116e-01,\n",
       "          4.53018993e-02,  6.11190647e-02, -1.85349777e-01,\n",
       "         -7.08621740e-02, -1.22774720e-01,  1.56791493e-01,\n",
       "         -2.76813507e-02,  2.31201947e-02, -1.14400484e-01,\n",
       "         -9.45350379e-02,  1.85581401e-01, -1.75479382e-01,\n",
       "          9.42436010e-02, -8.22617114e-03,  1.95388719e-01,\n",
       "          9.58819538e-02, -1.78379774e-01, -8.21486115e-03,\n",
       "         -1.33584350e-01, -1.40679926e-01,  7.75123686e-02,\n",
       "          8.01999122e-02,  1.09060839e-01,  1.93894044e-01,\n",
       "          5.38540334e-02,  1.98403791e-01, -5.93512952e-02,\n",
       "         -1.32160231e-01, -1.38852954e-01, -1.79851055e-02,\n",
       "          1.88877001e-01,  7.42303282e-02,  1.21150181e-01,\n",
       "         -1.18446641e-01,  3.93553823e-02,  1.85634002e-01,\n",
       "          1.72901154e-02, -2.08485410e-01,  1.60021588e-01,\n",
       "         -7.57371783e-02, -2.05237165e-01,  6.95730001e-02,\n",
       "          4.28397954e-03,  3.22099179e-02, -1.61005169e-01,\n",
       "         -1.43396705e-01, -4.05773371e-02,  6.76515251e-02,\n",
       "         -7.86784291e-02, -7.24022090e-03, -2.57029384e-02,\n",
       "         -6.14292324e-02,  5.02373129e-02, -1.61314726e-01,\n",
       "          1.97230861e-01, -1.76729903e-01, -1.36836410e-01,\n",
       "          1.83472738e-01, -7.98302591e-02,  1.27704158e-01,\n",
       "         -3.28656584e-02,  3.38747650e-02, -1.98453069e-01,\n",
       "          1.28297046e-01,  1.39162168e-01,  2.09002271e-01,\n",
       "          8.20189267e-02, -1.33395195e-03, -1.38254166e-01,\n",
       "         -1.73195660e-01, -1.30136728e-01, -1.47231638e-01,\n",
       "         -1.84734747e-01,  2.30850726e-02, -9.83422175e-02,\n",
       "         -1.05912432e-01, -7.49012232e-02, -9.13958773e-02,\n",
       "         -7.46713132e-02, -1.53325170e-01,  5.32322377e-02,\n",
       "         -1.05770260e-01,  3.12423706e-02,  1.52266875e-01,\n",
       "         -1.60699695e-01,  1.28755435e-01, -1.10822298e-01,\n",
       "         -1.27667084e-01,  1.87653586e-01, -1.99928895e-01,\n",
       "         -6.71499968e-02,  7.05073029e-02,  1.32022515e-01,\n",
       "          1.69857442e-02,  2.05802962e-01, -2.04119951e-01,\n",
       "         -1.47102937e-01, -1.89891964e-01, -7.53915906e-02,\n",
       "          1.91050768e-03,  1.99418887e-01],\n",
       "        [ 1.49430767e-01, -1.13444045e-01,  6.98696077e-03,\n",
       "          1.79025903e-01,  6.81504160e-02, -1.54896855e-01,\n",
       "          1.26966640e-01,  1.58470944e-01,  1.47483841e-01,\n",
       "          9.79341716e-02,  2.05644965e-03, -1.35424718e-01,\n",
       "         -1.98470205e-02, -2.10868523e-01, -1.48746312e-01,\n",
       "          2.48413533e-02,  1.56679317e-01,  7.52329677e-02,\n",
       "          1.52810112e-01,  1.57342479e-01, -7.67233223e-02,\n",
       "          6.65291399e-02, -1.59438640e-01,  1.01252198e-02,\n",
       "         -7.00493157e-02,  5.73402643e-03, -1.49294600e-01,\n",
       "         -4.10291553e-03,  4.27583605e-02, -9.16530192e-03,\n",
       "         -1.98851228e-02, -1.28121287e-01,  1.24934465e-02,\n",
       "         -8.21625143e-02, -6.25542253e-02, -1.52691618e-01,\n",
       "          1.80165514e-01, -8.58846456e-02, -2.39893943e-02,\n",
       "          1.38318345e-01, -9.73038524e-02,  6.35734200e-03,\n",
       "         -1.70296833e-01,  1.38231471e-01,  1.92279175e-01,\n",
       "         -1.08072408e-01,  9.85157490e-03, -3.83155048e-02,\n",
       "         -8.56791437e-03, -7.63481259e-02, -8.26394558e-03,\n",
       "          1.38805643e-01,  1.64064065e-01, -1.72445267e-01,\n",
       "         -5.03212959e-02,  5.25988191e-02, -1.07162639e-01,\n",
       "          1.41548023e-01,  2.23217607e-02, -1.79075435e-01,\n",
       "         -8.91800001e-02, -3.37442607e-02,  1.62417591e-02,\n",
       "         -6.25422150e-02, -1.78335488e-01,  1.84573397e-01,\n",
       "          1.26484737e-01, -4.64257896e-02, -3.84446979e-02,\n",
       "         -5.56432009e-02, -7.09229708e-03, -1.02655515e-01,\n",
       "         -3.68861407e-02,  1.52025416e-01, -3.99063528e-02,\n",
       "         -8.16143751e-02, -2.74708122e-02,  4.70822901e-02,\n",
       "         -1.70055389e-01, -7.61531293e-03,  1.55736789e-01,\n",
       "          1.85392424e-01, -9.23043340e-02, -2.02094823e-01,\n",
       "         -6.20846450e-03,  2.05713525e-01,  1.68963373e-02,\n",
       "          8.38735849e-02,  9.09224898e-02, -2.01606005e-01,\n",
       "          1.24614492e-01, -9.39447209e-02,  1.09775618e-01,\n",
       "         -2.02845216e-01,  5.77744693e-02, -1.91088036e-01,\n",
       "         -9.01001096e-02,  1.56142101e-01,  1.50269344e-01,\n",
       "         -1.42518476e-01,  4.78484184e-02, -1.12302758e-01,\n",
       "          1.24798879e-01,  2.51094401e-02,  4.09025997e-02,\n",
       "          1.79263160e-01, -4.57649380e-02, -1.85509354e-01,\n",
       "         -2.51759887e-02,  1.04558662e-01, -2.10447624e-01,\n",
       "          1.33365735e-01, -5.03181666e-02,  3.53627503e-02,\n",
       "          1.99743226e-01, -1.91832587e-01,  2.08170399e-01,\n",
       "          8.71798843e-02, -5.55151552e-02, -6.41237199e-03,\n",
       "          4.53166217e-02, -1.88141197e-01,  5.99332899e-02,\n",
       "         -9.75638703e-02,  3.47578079e-02,  4.54491377e-03,\n",
       "          2.25435942e-02, -2.01442495e-01],\n",
       "        [ 1.93490639e-01,  1.54770628e-01, -5.08926809e-03,\n",
       "         -1.09865107e-01, -5.60549200e-02,  9.23951417e-02,\n",
       "         -1.84878930e-01,  6.84834272e-02, -3.60572040e-02,\n",
       "          5.80289513e-02,  1.56158611e-01, -1.23628892e-01,\n",
       "         -1.97116494e-01, -1.62199825e-01,  1.28262952e-01,\n",
       "          1.43508956e-01, -2.37160027e-02,  1.67401657e-01,\n",
       "          1.99001715e-01,  1.34611920e-01,  8.21847171e-02,\n",
       "          1.59683153e-01, -2.79951394e-02,  1.52887061e-01,\n",
       "         -6.78104907e-02,  1.09971955e-01, -1.98655233e-01,\n",
       "         -1.23942137e-01, -1.65894240e-01,  5.25597185e-02,\n",
       "         -1.54559180e-01, -2.04442829e-01,  4.93336469e-02,\n",
       "         -1.53408945e-03,  6.46019131e-02,  1.96536973e-01,\n",
       "         -1.84259146e-01, -1.14683613e-01, -1.52030066e-01,\n",
       "          1.54745504e-01, -4.56731170e-02, -1.61745220e-01,\n",
       "          1.83828697e-01,  1.20141760e-01, -5.00725359e-02,\n",
       "          1.31313637e-01, -1.30807906e-02, -1.98126361e-01,\n",
       "         -1.47369415e-01,  1.06194451e-01, -3.37561071e-02,\n",
       "          7.05737025e-02, -1.90532744e-01,  2.24583298e-02,\n",
       "          1.28639475e-01,  1.96891025e-01, -1.55450895e-01,\n",
       "         -1.61875829e-01, -1.65765241e-01, -2.09396288e-01,\n",
       "          1.52146444e-01, -1.64346635e-01, -7.47159123e-02,\n",
       "         -1.19489603e-01, -1.26563489e-01, -3.47441435e-03,\n",
       "         -2.03536332e-01,  1.39182195e-01, -1.14184104e-01,\n",
       "         -7.98137635e-02, -1.26958549e-01, -1.70770675e-01,\n",
       "          1.66508570e-01, -1.47620261e-01, -1.10414207e-01,\n",
       "          1.50188074e-01, -2.09308043e-01, -9.22662914e-02,\n",
       "          1.85585126e-01,  1.44128129e-01,  1.51074275e-01,\n",
       "          1.10825315e-01,  9.28316861e-02, -1.16662018e-01,\n",
       "         -1.63635075e-01,  1.44174293e-01,  8.91800076e-02,\n",
       "          9.02156383e-02,  9.61424261e-02,  7.72382170e-02,\n",
       "         -1.66691616e-01,  8.83813351e-02,  1.71233714e-03,\n",
       "          7.99596310e-05,  3.31964642e-02,  1.59700021e-01,\n",
       "          2.01694205e-01, -2.06667438e-01, -1.16221130e-01,\n",
       "         -2.24108100e-02,  1.95793957e-02,  7.08117336e-02,\n",
       "         -2.03370556e-01, -1.23067431e-01,  1.70699880e-01,\n",
       "          8.41684788e-02,  2.04352364e-01,  8.84709209e-02,\n",
       "          5.86408526e-02, -1.92304045e-01,  9.89985466e-03,\n",
       "          1.97483525e-01,  7.35856742e-02,  1.28950581e-01,\n",
       "         -1.74392089e-01,  2.97472328e-02, -8.18106830e-02,\n",
       "          5.66697568e-02, -1.16085574e-01, -7.30554909e-02,\n",
       "          1.50920346e-01, -1.55198336e-01,  1.42031237e-01,\n",
       "          1.27990767e-01, -1.62489653e-01, -1.16145156e-01,\n",
       "         -1.86878294e-02,  7.65141696e-02]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 9.2464611e-02,  1.7189711e-02, -6.9273956e-02, ...,\n",
       "         -8.4140435e-02,  1.8439382e-02, -5.6202345e-02],\n",
       "        [-1.1395454e-02,  3.1264752e-02, -1.4998356e-01, ...,\n",
       "          6.6163227e-02,  3.5748124e-02, -5.1650219e-02],\n",
       "        [-9.5845297e-02, -7.7413946e-02, -1.1619812e-01, ...,\n",
       "         -1.6134098e-02,  1.3857724e-01,  4.6834350e-05],\n",
       "        ...,\n",
       "        [ 1.0168855e-01, -5.3257905e-02, -3.6910921e-03, ...,\n",
       "          3.1007677e-03,  1.3552143e-01,  1.4849292e-01],\n",
       "        [ 1.0083984e-01, -3.2570884e-02,  1.3250326e-01, ...,\n",
       "          9.2333719e-02, -5.2972801e-02, -1.4437124e-02],\n",
       "        [ 7.2974756e-02,  8.2103908e-02, -1.2513076e-01, ...,\n",
       "          7.1757063e-02, -1.2805530e-01,  1.2313579e-01]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 4.41238135e-02,  1.93990171e-02,  1.98331401e-01],\n",
       "        [ 1.42155543e-01,  1.23583242e-01, -2.60181874e-02],\n",
       "        [-3.41530889e-02,  1.44422725e-01, -3.61779034e-02],\n",
       "        [-1.30691230e-01, -1.60415798e-01, -4.01323736e-02],\n",
       "        [ 1.20283678e-01, -6.10926896e-02,  5.50990254e-02],\n",
       "        [ 9.38736349e-02,  6.03422523e-03,  2.91640610e-02],\n",
       "        [-1.54717982e-01,  5.92033565e-03, -3.54366004e-03],\n",
       "        [-1.14854604e-01,  1.40062720e-02,  1.69897541e-01],\n",
       "        [ 2.67264098e-02,  1.15531489e-01,  1.75068900e-01],\n",
       "        [-1.44524157e-01,  6.03321195e-03, -8.92358944e-02],\n",
       "        [-2.04015851e-01, -1.83774933e-01,  7.06124157e-02],\n",
       "        [ 1.32798091e-01,  8.99054110e-03,  1.88389912e-01],\n",
       "        [-1.60885841e-01, -1.34298518e-01,  1.52382717e-01],\n",
       "        [-1.90800980e-01, -2.07868725e-01,  1.88788608e-01],\n",
       "        [-5.86727858e-02,  1.10355869e-01,  4.98612374e-02],\n",
       "        [-8.53702128e-02, -1.27032042e-01, -5.72134554e-03],\n",
       "        [-1.30896702e-01,  1.53687283e-01,  1.62085161e-01],\n",
       "        [ 1.58692881e-01,  2.33106166e-02,  1.09962568e-01],\n",
       "        [ 8.73730332e-02,  9.15446430e-02,  2.91345716e-02],\n",
       "        [-3.78047824e-02, -1.74939305e-01,  7.62815028e-02],\n",
       "        [ 5.57679683e-02,  3.85506004e-02, -7.05430657e-02],\n",
       "        [ 1.28531501e-01,  1.42457187e-02,  1.30829558e-01],\n",
       "        [ 1.74303636e-01,  1.66145697e-01,  7.93881714e-03],\n",
       "        [ 1.18519440e-01,  1.39607474e-01, -2.12654114e-01],\n",
       "        [ 3.44899148e-02,  3.95252258e-02, -9.85858068e-02],\n",
       "        [ 1.57802776e-01,  1.76261142e-01,  8.69039744e-02],\n",
       "        [-1.13134108e-01,  1.47918925e-01,  4.61880714e-02],\n",
       "        [ 1.19438812e-01,  1.10201731e-01, -1.64196312e-01],\n",
       "        [ 1.27800539e-01, -1.42268509e-01,  6.23634905e-02],\n",
       "        [-1.48997098e-01,  2.00103685e-01, -1.02104396e-02],\n",
       "        [-1.84729710e-01, -4.75783497e-02, -1.42908275e-01],\n",
       "        [-1.23025186e-01, -2.04563096e-01,  4.09578532e-02],\n",
       "        [-1.29216015e-01,  4.31246012e-02, -2.16822177e-02],\n",
       "        [ 1.80255398e-01, -1.14738941e-03, -1.90204039e-01],\n",
       "        [ 6.87115341e-02,  3.48993838e-02, -8.94996375e-02],\n",
       "        [-1.26505673e-01, -2.04884440e-01, -2.07220092e-01],\n",
       "        [-3.43790799e-02,  7.54660219e-02,  9.11165625e-02],\n",
       "        [ 1.24750212e-01, -1.66603193e-01, -1.71700403e-01],\n",
       "        [ 1.46058872e-01, -7.81236887e-02, -4.98820543e-02],\n",
       "        [-1.14682041e-01, -2.08682567e-01,  2.09066883e-01],\n",
       "        [ 1.36509463e-01, -2.14954615e-02,  1.21171728e-01],\n",
       "        [ 1.45047680e-01, -1.76690415e-01,  5.09139895e-03],\n",
       "        [-1.83296785e-01, -4.38689590e-02, -1.70415968e-01],\n",
       "        [-1.80327758e-01, -7.16976523e-02,  8.67519230e-02],\n",
       "        [ 8.30058753e-03,  6.40576333e-02, -1.72312036e-01],\n",
       "        [ 9.05113667e-02, -1.63170099e-01,  1.38597712e-01],\n",
       "        [ 8.29987228e-03, -1.75699875e-01, -1.68701530e-01],\n",
       "        [-1.93157092e-01,  2.02171370e-01, -1.42710894e-01],\n",
       "        [-3.72503400e-02, -7.20495582e-02,  4.71260995e-02],\n",
       "        [ 6.42441213e-03,  1.70616433e-01,  1.77042678e-01],\n",
       "        [-4.82714176e-03,  1.02822334e-02,  1.85896590e-01],\n",
       "        [-1.31364495e-01,  7.44489878e-02,  2.52105594e-02],\n",
       "        [ 8.84156078e-02,  2.10347548e-01,  9.17790681e-02],\n",
       "        [ 9.76534933e-02,  1.50901183e-01,  9.46664363e-02],\n",
       "        [ 1.50731966e-01,  1.64107069e-01,  1.97572157e-01],\n",
       "        [ 4.70693856e-02, -1.68071434e-01,  3.70372087e-02],\n",
       "        [ 9.21796262e-03,  1.74011990e-01, -1.41878381e-01],\n",
       "        [-1.00888900e-01, -7.74747580e-02, -1.08429223e-01],\n",
       "        [ 1.36651471e-01,  1.47832796e-01, -1.05496280e-01],\n",
       "        [ 1.92822739e-01, -1.91868663e-01, -1.29859120e-01],\n",
       "        [-2.16437876e-02,  8.91348571e-02, -1.84744507e-01],\n",
       "        [-2.30850875e-02,  1.51820585e-01, -1.48426473e-01],\n",
       "        [ 5.11308759e-02, -4.95094210e-02, -5.64310700e-02],\n",
       "        [ 7.89314508e-05,  3.27552855e-03,  5.06255776e-02],\n",
       "        [ 9.61941630e-02, -2.10282385e-01,  5.88654131e-02],\n",
       "        [ 2.13241771e-01,  3.00559253e-02, -1.04591355e-01],\n",
       "        [-1.87565088e-02,  8.94830674e-02, -7.34617710e-02],\n",
       "        [-1.85419247e-01, -7.82560557e-02, -8.66088420e-02],\n",
       "        [-6.20158762e-02,  1.20598719e-01, -3.98912728e-02],\n",
       "        [ 4.97222096e-02, -1.85665935e-02,  1.30403042e-03],\n",
       "        [-1.03741795e-01,  1.79035291e-01,  5.13905287e-03],\n",
       "        [-9.97868255e-02,  5.45888394e-02,  1.51960030e-01],\n",
       "        [-1.27682567e-01, -1.03189245e-01,  1.20908991e-01],\n",
       "        [ 8.70229155e-02, -2.16156244e-02,  1.19781241e-01],\n",
       "        [-1.40728503e-01,  9.24660712e-02,  2.12973848e-01],\n",
       "        [ 1.96707651e-01, -1.89593375e-01, -1.13424234e-01],\n",
       "        [-1.94082424e-01,  7.57760555e-02,  1.80810139e-01],\n",
       "        [-6.20484352e-05, -8.79115462e-02,  5.05772233e-03],\n",
       "        [ 3.40761542e-02, -9.97521281e-02,  2.53884345e-02],\n",
       "        [ 1.45962760e-01,  1.86839536e-01, -1.17744081e-01],\n",
       "        [ 6.41250163e-02, -1.02732629e-01, -1.89952850e-01],\n",
       "        [ 1.20714739e-01, -3.05506587e-02,  1.01046428e-01],\n",
       "        [ 1.07918158e-01, -1.87309563e-01,  9.11353379e-02],\n",
       "        [ 7.58177489e-02,  1.95793316e-01, -1.94530487e-02],\n",
       "        [-1.84356719e-02,  9.78925377e-02,  5.79311401e-02],\n",
       "        [-2.02593595e-01,  6.09432310e-02,  7.82633573e-02],\n",
       "        [-2.09760055e-01, -1.45590693e-01,  6.40846640e-02],\n",
       "        [-1.23391032e-01, -5.06740808e-03,  6.18648380e-02],\n",
       "        [ 9.03685838e-02, -3.94676775e-02, -9.27797556e-02],\n",
       "        [-2.52231658e-02,  1.92191407e-01,  2.72497535e-03],\n",
       "        [ 3.72627527e-02, -8.55088979e-02,  1.91851780e-01],\n",
       "        [ 3.37108672e-02,  4.29203957e-02,  1.55030921e-01],\n",
       "        [-1.27861053e-01, -1.57791197e-01,  1.01330891e-01],\n",
       "        [ 1.58348009e-01,  1.82897463e-01,  2.87324190e-04],\n",
       "        [ 1.03954867e-01,  1.44067481e-01,  2.04769388e-01],\n",
       "        [ 2.13122591e-01, -6.92999512e-02, -7.06658214e-02],\n",
       "        [-9.97589082e-02,  2.25502998e-02,  1.38570353e-01],\n",
       "        [ 7.99169987e-02,  1.62186310e-01,  3.51971090e-02],\n",
       "        [-1.12674169e-01,  6.92419708e-03,  6.20351583e-02],\n",
       "        [-9.86689329e-03, -1.64571196e-01,  7.20816702e-02],\n",
       "        [ 1.67869315e-01, -1.96851984e-01, -2.63663828e-02],\n",
       "        [-7.13025630e-02, -1.00699350e-01,  1.16854861e-01],\n",
       "        [-8.75523388e-02, -1.23436898e-01,  2.09001109e-01],\n",
       "        [-3.69162410e-02, -1.35865241e-01,  1.49735466e-01],\n",
       "        [-5.66009283e-02, -1.53403834e-01,  1.42159477e-01],\n",
       "        [ 7.57024139e-02,  1.50034979e-01,  4.27348167e-02],\n",
       "        [ 2.08033219e-01,  6.21592551e-02, -1.07496850e-01],\n",
       "        [ 1.75457194e-01, -7.74686933e-02, -4.69754338e-02],\n",
       "        [-3.01737338e-02,  4.20178920e-02, -9.52381268e-02],\n",
       "        [ 1.95104852e-01,  6.59856200e-03,  3.34728807e-02],\n",
       "        [-2.09284037e-01,  7.36853629e-02, -1.94080740e-01],\n",
       "        [-1.61775142e-01,  3.66913229e-02,  1.13246515e-01],\n",
       "        [-5.86090535e-02,  1.85764745e-01, -1.36809230e-01],\n",
       "        [-2.11912990e-01,  4.29778248e-02,  1.92598060e-01],\n",
       "        [ 1.13671198e-01, -7.33032972e-02, -7.82378912e-02],\n",
       "        [-3.34263593e-02, -8.89919400e-02, -1.29590631e-01],\n",
       "        [-1.79208815e-03,  1.41468719e-01, -1.89761609e-01],\n",
       "        [-9.12238657e-03,  8.14606100e-02, -9.16591287e-03],\n",
       "        [-3.94594669e-03, -1.98035806e-01,  1.61034063e-01],\n",
       "        [-5.16006202e-02,  1.25803486e-01,  3.08292955e-02],\n",
       "        [ 1.34325728e-01, -4.62505817e-02,  1.79973349e-01],\n",
       "        [-6.99944496e-02, -7.94192553e-02,  1.13058373e-01],\n",
       "        [ 4.95033413e-02,  1.56401441e-01, -1.35491118e-01],\n",
       "        [-8.09500962e-02, -1.25214458e-04,  1.75940499e-01],\n",
       "        [ 2.22725719e-02,  1.95194915e-01, -1.75917804e-01],\n",
       "        [ 6.06391281e-02,  8.52486640e-02,  4.69620526e-03],\n",
       "        [ 1.02031484e-01, -2.70068645e-04,  1.47267446e-01],\n",
       "        [ 7.54170865e-02, -2.68988758e-02, -1.73129663e-01]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_agent.policy_old.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(44,), dtype=float32, numpy=\n",
       "array([-1.1930662e+10, -1.2127937e+10, -1.2337765e+10, -1.2560848e+10,\n",
       "       -1.2798065e+10, -1.3050268e+10, -1.3318447e+10, -1.3603593e+10,\n",
       "       -1.3906797e+10, -1.4229164e+10, -1.4571965e+10, -1.4936414e+10,\n",
       "       -1.5323933e+10, -1.5735954e+10, -1.6174060e+10, -1.6639864e+10,\n",
       "       -1.7135172e+10, -1.7661780e+10, -1.8221703e+10, -1.8817044e+10,\n",
       "       -1.9450073e+10, -2.0123132e+10, -2.0838771e+10, -2.1599689e+10,\n",
       "       -2.2408741e+10, -2.3269007e+10, -2.4183667e+10, -2.5156188e+10,\n",
       "       -2.6190240e+10, -2.7289723e+10, -2.8458750e+10, -2.9701761e+10,\n",
       "       -3.1023380e+10, -3.2428612e+10, -3.3922685e+10, -3.5511341e+10,\n",
       "       -3.7200531e+10, -3.8996558e+10, -4.0906224e+10, -4.2936689e+10,\n",
       "       -4.5095608e+10, -4.7391105e+10, -4.9831846e+10, -5.2426977e+10],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = tf.convert_to_tensor(np.zeros((1, state_dim)), dtype=tf.float32)\n",
    "_ = ppo_agent.policy(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: policy shape (6, 128), policy_old shape (6, 128)\n",
      "Layer 1: policy shape (128,), policy_old shape (128,)\n",
      "Layer 2: policy shape (128, 128), policy_old shape (128, 128)\n",
      "Layer 3: policy shape (128,), policy_old shape (128,)\n",
      "Layer 4: policy shape (128, 3), policy_old shape (128, 3)\n",
      "Layer 5: policy shape (3,), policy_old shape (3,)\n"
     ]
    }
   ],
   "source": [
    "for i, (w1, w2) in enumerate(zip(policy_weights, policy_old_weights)):\n",
    "    print(f\"Layer {i}: policy shape {w1.shape}, policy_old shape {w2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 9.99998764e-11 9.99998764e-11 9.99998764e-11 9.99998764e-11\n",
      " 1.00000064e-10 1.00000064e-10 1.00000064e-10 1.00000064e-10], shape=(44,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "states, actions, logprobs, rewards, _, done_vals = experiences\n",
    "rewards = []\n",
    "discounted_reward = 0\n",
    "for reward, is_done in zip(reversed(rewards), reversed(done_vals)):\n",
    "    if is_done:\n",
    "        discounted_reward = 0\n",
    "    discounted_reward = reward + (gamma * discounted_reward)\n",
    "    rewards.insert(0, discounted_reward)\n",
    "\n",
    "\n",
    "\n",
    "rewards, old_states, old_actions, old_logprobs, *_ = experiences\n",
    "rewards         = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "old_states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "old_actions     = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "old_logprobs    = tf.convert_to_tensor(logprobs, dtype=tf.float32)\n",
    "values          = ppo_agent.critic(old_states).numpy()\n",
    "values          = np.append(values, 0)  # Append 0 for the last value\n",
    "\n",
    "advantages = compute_gae(rewards, values, ppo_agent.gamma, ppo_agent.lam)\n",
    "advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    probs = ppo_agent.policy(old_states)\n",
    "    probs = tf.clip_by_value(probs, 1e-10, 1.0)\n",
    "    logprobs = tf.math.log(tf.gather(probs, old_actions, axis=1, batch_dims=1))\n",
    "    policy_old_outputs = ppo_agent.policy(old_states)\n",
    "    policy_old_outputs = tf.clip_by_value(policy_old_outputs, 1e-10, 1.0)\n",
    "    dist_entropy = -tf.reduce_sum(policy_old_outputs * tf.math.log(policy_old_outputs), axis=1) #-tf.reduce_sum(ppo_agent.policy(old_states) * tf.math.log(ppo_agent.policy(old_states)), axis=1)\n",
    "    ratios = tf.exp(logprobs - old_logprobs)\n",
    "    print(ratios)\n",
    "    # print(advantages)\n",
    "    surr1 = tf.expand_dims(ratios, axis=1) * advantages\n",
    "    surr2 = tf.expand_dims(tf.clip_by_value(ratios, 1-ppo_agent.eps_clip, 1+ppo_agent.eps_clip), axis=1) * advantages\n",
    "    loss = -tf.reduce_mean(tf.minimum(surr1, surr2)) + 0.01 * tf.reduce_mean(dist_entropy)\n",
    "    value_loss = tf.reduce_mean(tf.square(rewards - ppo_agent.critic(old_states)))\n",
    "    total_loss = loss + value_loss\n",
    "\n",
    "grads = tape.gradient(total_loss, ppo_agent.policy.trainable_variables + ppo_agent.critic.trainable_variables)\n",
    "ppo_agent.optimizer.apply_gradients(zip(grads, ppo_agent.policy.trainable_variables + ppo_agent.critic.trainable_variables))\n",
    "\n",
    "\n",
    "policy_weights = ppo_agent.policy.get_weights()\n",
    "policy_old_weights = ppo_agent.policy_old.get_weights()\n",
    "\n",
    "# for i, (w1, w2) in enumerate(zip(policy_weights, policy_old_weights)):\n",
    "#     print(f\"Layer {i}: policy shape {w1.shape}, policy_old shape {w2.shape}\")\n",
    "\n",
    "\n",
    "ppo_agent.policy_old.set_weights(ppo_agent.policy.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(199, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_agent.policy_old(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: policy shape (6, 128), policy_old shape (6, 128) grad shape (6, 128)\n",
      "Layer 1: policy shape (128,), policy_old shape (128,) grad shape (128,)\n",
      "Layer 2: policy shape (128, 128), policy_old shape (128, 128) grad shape (128, 128)\n",
      "Layer 3: policy shape (128,), policy_old shape (128,) grad shape (128,)\n",
      "Layer 4: policy shape (128, 3), policy_old shape (128, 3) grad shape (128, 3)\n",
      "Layer 5: policy shape (3,), policy_old shape (3,) grad shape (3,)\n"
     ]
    }
   ],
   "source": [
    "for i, (w1, w2, g1) in enumerate(zip(policy_weights, policy_old_weights, grads)):\n",
    "    print(f\"Layer {i}: policy shape {w1.shape}, policy_old shape {w2.shape} grad shape {g1.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 6)\n",
      "Shape after dense1: (1, 128)\n",
      "Shape after dense2: (1, 128)\n",
      "Shape after logits: (1, 3)\n",
      "Output: [[0.26531073 0.27612218 0.45856702]]\n",
      "Original state shape: (6,)\n",
      "State shape after expand_dims: (1, 6)\n",
      "Input shape: (1, 6)\n",
      "Shape after dense1: (1, 128)\n",
      "Shape after dense2: (1, 128)\n",
      "Shape after logits: (1, 3)\n",
      "Output: [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dense2 = layers.Dense(128, activation='relu')\n",
    "        self.logits = layers.Dense(action_dim, activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        print(f\"Input shape: {inputs.shape}\")\n",
    "        x = self.dense1(inputs)\n",
    "        print(f\"Shape after dense1: {x.shape}\")\n",
    "        x = self.dense2(x)\n",
    "        print(f\"Shape after dense2: {x.shape}\")\n",
    "        x = self.logits(x)\n",
    "        print(f\"Shape after logits: {x.shape}\")\n",
    "        return tf.nn.softmax(x)\n",
    "\n",
    "# Example usage\n",
    "state_dim = 6  # Example state dimension\n",
    "action_dim = 3  # Number of actions\n",
    "\n",
    "policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "example_input = tf.random.normal((1, state_dim))  # Add batch dimension\n",
    "output = policy_net(example_input)\n",
    "\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Ensure the input has the correct shape when calling the policy network\n",
    "state = states[0] #tf.random.normal((state_dim,))  # Example state\n",
    "print(f\"Original state shape: {state.shape}\")\n",
    "state = tf.expand_dims(state, 0)  # Add batch dimension\n",
    "print(f\"State shape after expand_dims: {state.shape}\")\n",
    "output = policy_net(state)\n",
    "\n",
    "print(f\"Output: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: PPO, Actor-Critic Style\n",
    "\n",
    "1. **for** iteration = 1, 2, . . .\n",
    "   - **for** actor = 1, 2, . . . , N\n",
    "     - Run policy πθold in environment for T timesteps\n",
    "     - Compute advantage estimates Aˆ1, . . . , AˆT\n",
    "   - Optimize surrogate L wrt θ, with K epochs and minibatch size M ≤ NT\n",
    "   - θold ← θ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "findesk_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
